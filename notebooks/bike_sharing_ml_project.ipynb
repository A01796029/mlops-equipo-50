{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f788e2",
   "metadata": {},
   "source": [
    "### 1. IMPORTACIÓN DE LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9309b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761524a9",
   "metadata": {},
   "source": [
    "### 2. CARGA DE LOS DATOS (DE MOMENTO CONSIDERANDO UN .CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57dad4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Carga un conjunto de datos desde una ruta de archivo CSV.\n",
    "\n",
    "    Esta función encapsula la lógica de lectura de datos con Pandas,\n",
    "    incluyendo un manejo básico de errores si el archivo no se encuentra.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): La ruta al archivo .csv que se va a cargar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Un DataFrame de Pandas con los datos cargados,\n",
    "        o None si ocurre un error (ej. archivo no encontrado).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intenta leer el archivo CSV y lo carga en un DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Datos cargados exitosamente desde: {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        # Manejo de error si el archivo no existe en la ruta especificada\n",
    "        print(f\"Error: El archivo no fue encontrado en la ruta: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Manejo de otros posibles errores durante la carga\n",
    "        print(f\"Ocurrió un error inesperado al cargar el archivo: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175ed98",
   "metadata": {},
   "source": [
    "### 3. DIAGNÓSTICO INICIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18d824d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_overview(df: pd.DataFrame, df_name: str = \"DataFrame\") -> None:\n",
    "    \"\"\"\n",
    "    Imprime un resumen completo y diagnóstico de un DataFrame.\n",
    "\n",
    "    Incluye dimensiones, tipos de datos, estadísticas descriptivas,\n",
    "    conteo de duplicados y porcentaje de valores nulos.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame que se va a analizar.\n",
    "        df_name (str): Un nombre opcional para el DataFrame que se mostrará en los reportes.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===============ANÁLISIS EXPLORATORIO RÁPIDO PARA: '{df_name}'===============\")\n",
    "\n",
    "    # 1. Dimensiones del DataFrame\n",
    "    print(f\"\\n**Dimensiones:** {df.shape[0]} filas y {df.shape[1]} columnas.\")\n",
    "\n",
    "    # 2. Tipos de datos y valores no nulos\n",
    "    print(\"\\n**Tipos de Datos y Valores No Nulos:**\")\n",
    "    df.info()\n",
    "\n",
    "    # 3. Estadísticas Descriptivas para variables numéricas\n",
    "    print(\"\\n**Estadísticas Descriptivas (Numéricas):**\")\n",
    "    # Usamos .T para transponer la tabla y hacerla más legible\n",
    "    print(df.describe().T)\n",
    "\n",
    "    # 4. Conteo de filas duplicadas\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\n**Filas Duplicadas:** {duplicates} filas duplicadas encontradas.\")\n",
    "\n",
    "    # 5. Porcentaje de Valores Nulos por columna\n",
    "    null_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    null_info = null_percentage[null_percentage > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if not null_info.empty:\n",
    "        print(\"\\n**Porcentaje de Valores Nulos (>0%):**\")\n",
    "        print(null_info)\n",
    "    else:\n",
    "        print(\"\\n**No se encontraron valores nulos.**\")\n",
    "    \n",
    "    print(\"\\nFin del análisis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c8d8e",
   "metadata": {},
   "source": [
    "### 4. LIMPIEZA DE LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afb0e9",
   "metadata": {},
   "source": [
    "##### Validación del esquema. Eliminación de columnas no identificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49d0d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary_columns(df: pd.DataFrame, valid_columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Elimina las columnas de un DataFrame que no están en una lista de columnas válidas.\n",
    "\n",
    "    Esta función es útil para asegurar que el DataFrame solo contenga las columnas\n",
    "    esperadas según el esquema definido.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame a limpiar.\n",
    "        valid_columns (list): Una lista de strings con los nombres de las\n",
    "                              columnas que deben permanecer.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nuevo DataFrame que solo contiene las columnas válidas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identifica las columnas actuales del DataFrame\n",
    "    current_columns = df.columns.tolist()\n",
    "    \n",
    "    # Encuentra las columnas que están en el DataFrame pero no en la lista de válidas\n",
    "    cols_to_drop = [col for col in current_columns if col not in valid_columns]\n",
    "\n",
    "    if cols_to_drop:\n",
    "        print(f\"\\nColumnas a eliminar: {cols_to_drop}\")\n",
    "        # Elimina las columnas identificadas y devuelve una copia del df modificado\n",
    "        df_cleaned = df.drop(columns=cols_to_drop)\n",
    "        print(\"Columnas innecesarias eliminadas.\")\n",
    "    else:\n",
    "        print(\"No se encontraron columnas innecesarias. El esquema es correcto.\")\n",
    "        df_cleaned = df.copy() # Devuelve una copia para mantener la consistencia\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603e467",
   "metadata": {},
   "source": [
    "##### Conversión del tipo correcto de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f3e0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_initial_data_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Corrige los tipos de datos de columnas específicas a numérico y fecha.\n",
    "    Las categóricas de momento igual se consideran como numéricas, para facilitar el manejo de inválidos.\n",
    "\n",
    "    Usa 'errors=coerce' para convertir valores no válidos en NaN,\n",
    "    facilitando su manejo posterior.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame a procesar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nuevo DataFrame con los tipos de datos corregidos.\n",
    "    \"\"\"\n",
    "\n",
    "    df_corrected = df.copy()\n",
    "    print(\"\\nIniciando corrección de tipos de datos...\")\n",
    "\n",
    "    # Define los grupos de columnas\n",
    "    numeric_cols = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt',\n",
    "                    'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "    \n",
    "    # 1. Procesa columnas numéricas\n",
    "    for col in numeric_cols:\n",
    "        # Omite si la columna no existe (por si fue eliminada antes)\n",
    "        if col in df_corrected.columns:\n",
    "            df_corrected[col] = pd.to_numeric(df_corrected[col], errors='coerce')\n",
    "\n",
    "    # 2. Procesa columna de fecha\n",
    "    if 'dteday' in df_corrected.columns:\n",
    "        # Usamos format='mixed' para manejar explícitamente los formatos múltiples\n",
    "        df_corrected['dteday'] = pd.to_datetime(\n",
    "            df_corrected['dteday'], \n",
    "            errors='coerce', \n",
    "            format='mixed' # Esta es la solución\n",
    "        )\n",
    "        \n",
    "    print(\"Tipos de datos corregidos de forma semántica.\")\n",
    "    return df_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19660c0",
   "metadata": {},
   "source": [
    "##### Manejo de valores no válidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b39495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_invalid_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Valida los datos contra un conjunto de reglas y convierte los inválidos a NaN.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame con tipos de datos ya corregidos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nuevo DataFrame con los valores inválidos convertidos a NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    df_validated = df.copy()\n",
    "    print(\"\\nIniciando validación de valores...\")\n",
    "\n",
    "    # Diccionario de reglas de validación\n",
    "    # Para categóricas: lista de valores permitidos\n",
    "    # Para numéricas: tupla con (valor_mínimo, valor_máximo)\n",
    "    validation_rules = {\n",
    "        'dteday': (pd.to_datetime('2011-01-01'), pd.to_datetime('2012-12-31')),\n",
    "        'season': [1, 2, 3, 4],\n",
    "        'yr': [0, 1],\n",
    "        'mnth': list(range(1, 13)),\n",
    "        'hr': list(range(0, 24)),\n",
    "        'holiday': [0, 1],\n",
    "        'weekday': list(range(0, 7)),\n",
    "        'workingday': [0, 1],\n",
    "        'weathersit': [1, 2, 3, 4],\n",
    "        'hum': (0.0, 1.0),\n",
    "        'windspeed': (0.0, 1.0),\n",
    "        'cnt': (0, float('inf')) # El conteo no puede ser negativo\n",
    "    }\n",
    "\n",
    "    for column, rule in validation_rules.items():\n",
    "        if column in df_validated.columns:\n",
    "            # Conteo inicial de nulos para reporte\n",
    "            initial_nulls = df_validated[column].isnull().sum()\n",
    "\n",
    "            # Validación para variables categóricas (regla es una lista)\n",
    "            if isinstance(rule, list):\n",
    "                invalid_mask = ~df_validated[column].isin(rule)\n",
    "            # Validación para variables numéricas (regla es una tupla)\n",
    "            elif isinstance(rule, tuple):\n",
    "                min_val, max_val = rule\n",
    "                invalid_mask = (df_validated[column] < min_val) | (df_validated[column] > max_val)\n",
    "            \n",
    "            # Reemplaza los valores que no cumplen la regla con NaN\n",
    "            df_validated.loc[invalid_mask, column] = np.nan\n",
    "            \n",
    "            # Reporta cuántos valores inválidos se encontraron y corrigieron\n",
    "            final_nulls = df_validated[column].isnull().sum()\n",
    "            newly_invalid = final_nulls - initial_nulls\n",
    "            if newly_invalid > 0:\n",
    "                print(f\"  -> Columna '{column}': {newly_invalid} valores inválidos convertidos a NaN.\")\n",
    "\n",
    "    print(\"Validación de valores completada.\")\n",
    "    return df_validated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842592d",
   "metadata": {},
   "source": [
    "##### Imputación de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4cfde046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_season(date_obj: pd.Timestamp) -> int:\n",
    "    \"\"\"\n",
    "    Convierte una fecha completa a la estación correspondiente de forma precisa,\n",
    "    considerando los días de corte (solsticios y equinoccios).\n",
    "    Dataset: 1:invierno, 2:primavera, 3:verano, 4:otoño.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_obj):\n",
    "        return np.nan\n",
    "        \n",
    "    month = date_obj.month\n",
    "    day = date_obj.day\n",
    "\n",
    "    # Invierno: Desde 21 de Dic hasta 20 de Mar\n",
    "    if (month == 12 and day >= 21) or (month in [1, 2]) or (month == 3 and day < 21):\n",
    "        return 1\n",
    "    # Primavera: Desde 21 de Mar hasta 20 de Jun\n",
    "    elif (month == 3 and day >= 21) or (month in [4, 5]) or (month == 6 and day < 21):\n",
    "        return 2\n",
    "    # Verano: Desde 21 de Jun hasta 22 de Sep\n",
    "    elif (month == 6 and day >= 21) or (month in [7, 8]) or (month == 9 and day < 23):\n",
    "        return 3\n",
    "    # Otoño: Desde 23 de Sep hasta 20 de Dic\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9a3b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gestiona valores nulos: primero elimina filas irrecuperables y luego\n",
    "    imputa los nulos restantes de forma contextual.\n",
    "    \"\"\"\n",
    "\n",
    "    df_processed = df.copy()\n",
    "    print(\"\\nIniciando manejo de valores nulos...\")\n",
    "    \n",
    "    initial_rows = len(df_processed)\n",
    "    \n",
    "    # --- ESTRATEGIA 1: ELIMINACIÓN DE FILAS CRÍTICAS ---\n",
    "    print(\"\\n  -> Paso 1: Eliminando filas con datos críticos faltantes...\")\n",
    "    critical_cols = ['dteday', 'hr', 'holiday', 'workingday', 'casual', 'registered', 'cnt']\n",
    "    df_processed.dropna(subset=critical_cols, inplace=True)\n",
    "    rows_deleted = initial_rows - len(df_processed)\n",
    "    if rows_deleted > 0:\n",
    "        print(f\"Se eliminaron {rows_deleted} filas.\")\n",
    "\n",
    "    # --- ESTRATEGIA 2: IMPUTACIÓN CONTEXTUAL ---\n",
    "    print(\"\\n  -> Paso 2: Imputando valores restantes de forma contextual...\")\n",
    "    \n",
    "    # Derivación por fecha (solo para celdas vacías)\n",
    "    mask_yr = df_processed['yr'].isna()\n",
    "    df_processed.loc[mask_yr, 'yr'] = df_processed.loc[mask_yr, 'dteday'].dt.year - 2011\n",
    "    \n",
    "    mask_mnth = df_processed['mnth'].isna()\n",
    "    df_processed.loc[mask_mnth, 'mnth'] = df_processed.loc[mask_mnth, 'dteday'].dt.month\n",
    "    \n",
    "    mask_weekday = df_processed['weekday'].isna()\n",
    "    df_processed.loc[mask_weekday, 'weekday'] = (df_processed.loc[mask_weekday, 'dteday'].dt.weekday + 1) % 7\n",
    "    \n",
    "    mask_season = df_processed['season'].isna()\n",
    "    df_processed.loc[mask_season, 'season'] = df_processed.loc[mask_season, 'dteday'].apply(date_to_season)\n",
    "\n",
    "    # Imputación estadística para el clima (sin cambios)\n",
    "    weather_cols = ['weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "    for column in weather_cols:\n",
    "        if column in df_processed.columns and df_processed[column].isnull().any():\n",
    "            median_val = df_processed[column].median()\n",
    "            df_processed[column] = df_processed[column].fillna(median_val)\n",
    "\n",
    "    print(\"Proceso de imputación de nulos finalizado.\")\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f513c1",
   "metadata": {},
   "source": [
    "##### Revisión de registros con inconsistencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d81ffc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inconsistencies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Verifica y corrige inconsistencias lógicas en el DataFrame.\n",
    "\n",
    "    - Regla 1: Valida 'season', 'yr', 'mnth', 'weekday' contra 'dteday'.\n",
    "    - Regla 2: Valida 'workingday' contra 'weekday' y 'holiday'.\n",
    "    - Regla 3: Valida 'cnt' = 'casual' + 'registered' y elimina filas inconsistentes.\n",
    "    \"\"\"\n",
    "\n",
    "    df_consistent = df.copy()\n",
    "    print(\"\\nVerificando inconsistencias lógicas...\")\n",
    "\n",
    "    # --- Regla 1: Consistencia de variables de tiempo vs. 'dteday' ---\n",
    "    print(\"\\n  -> Validando consistencia de fecha (yr, mnth, season, weekday)...\")\n",
    "    \n",
    "    # Se calculan los valores correctos a partir de la fecha\n",
    "    correct_yr = df_consistent['dteday'].dt.year - 2011\n",
    "    correct_mnth = df_consistent['dteday'].dt.month\n",
    "    correct_weekday = (df_consistent['dteday'].dt.weekday + 1) % 7\n",
    "    correct_season = df_consistent['dteday'].apply(date_to_season)\n",
    "\n",
    "    # Se comparan y corrigen\n",
    "    yr_inconsistencies = (df_consistent['yr'] != correct_yr).sum()\n",
    "    mnth_inconsistencies = (df_consistent['mnth'] != correct_mnth).sum()\n",
    "    weekday_inconsistencies = (df_consistent['weekday'] != correct_weekday).sum()\n",
    "    season_inconsistencies = (df_consistent['season'] != correct_season).sum()\n",
    "    \n",
    "    df_consistent['yr'] = correct_yr\n",
    "    df_consistent['mnth'] = correct_mnth\n",
    "    df_consistent['weekday'] = correct_weekday\n",
    "    df_consistent['season'] = correct_season\n",
    "    \n",
    "    print(f\"    - Corregidas {yr_inconsistencies} inconsistencias en 'yr'.\")\n",
    "    print(f\"    - Corregidas {mnth_inconsistencies} inconsistencias en 'mnth'.\")\n",
    "    print(f\"    - Corregidas {weekday_inconsistencies} inconsistencias en 'weekday'.\")\n",
    "    print(f\"    - Corregidas {season_inconsistencies} inconsistencias en 'season'.\")\n",
    "    \n",
    "    # --- Regla 2: Consistencia de 'workingday' ---\n",
    "    print(\"\\n  -> Validando consistencia de 'workingday'...\")\n",
    "    \n",
    "    # Se calcula el valor correcto: no es fin de semana (0 o 6) Y no es festivo (0)\n",
    "    correct_workingday = ((df_consistent['weekday'].isin([0, 6])) | (df_consistent['holiday'] == 1)).apply(lambda x: 0 if x else 1)\n",
    "    \n",
    "    workingday_inconsistencies = (df_consistent['workingday'] != correct_workingday).sum()\n",
    "    df_consistent['workingday'] = correct_workingday\n",
    "    print(f\"    - Corregidas {workingday_inconsistencies} inconsistencias en 'workingday'.\")\n",
    "\n",
    "    # --- Regla 3: Consistencia de los conteos ('cnt') ---\n",
    "    print(\"\\n  -> Validando consistencia de 'cnt' vs 'casual' + 'registered'...\")\n",
    "    \n",
    "    # Se identifican las filas donde la suma no cuadra\n",
    "    inconsistent_sum_mask = df_consistent['cnt'] != (df_consistent['casual'] + df_consistent['registered'])\n",
    "    \n",
    "    rows_to_drop = inconsistent_sum_mask.sum()\n",
    "    \n",
    "    if rows_to_drop > 0:\n",
    "        df_consistent = df_consistent[~inconsistent_sum_mask]\n",
    "        print(f\"    - Se eliminaron {rows_to_drop} filas por inconsistencia en la suma de conteos.\")\n",
    "    else:\n",
    "        print(\"    - No se encontraron inconsistencias en la suma de conteos.\")\n",
    "\n",
    "    print(\"\\nVerificación de inconsistencias completada.\")\n",
    "    return df_consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609554d1",
   "metadata": {},
   "source": [
    "##### Eliminación de registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87e518b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encuentra y elimina filas completamente duplicadas en el DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\nVerificando filas duplicadas...\")\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # Se eliminan las filas duplicadas\n",
    "    df_unique = df.drop_duplicates()\n",
    "    \n",
    "    final_rows = len(df_unique)\n",
    "    rows_dropped = initial_rows - final_rows\n",
    "    \n",
    "    if rows_dropped > 0:\n",
    "        print(f\"Se eliminaron {rows_dropped} filas duplicadas.\")\n",
    "    else:\n",
    "        print(\"No se encontraron filas duplicadas.\")\n",
    "        \n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d99616",
   "metadata": {},
   "source": [
    "##### Conversión final de columnas categóricas y numéricas enteras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87d2a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_data_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Paso Final: Convierte las columnas a sus tipos semánticos finales (category, int).\n",
    "    Se ejecuta después de que toda la limpieza e imputación han sido completadas.\n",
    "    \"\"\"\n",
    "    df_finalized = df.copy()\n",
    "    print(\"\\nPuliendo los tipos de datos finales...\")\n",
    "\n",
    "    # Define qué columnas deben ser categóricas y cuáles de conteo (enteros)\n",
    "    categorical_cols = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "    count_cols = ['casual', 'registered', 'cnt']\n",
    "\n",
    "    # Convierte las columnas categóricas\n",
    "    for col in categorical_cols:\n",
    "        if col in df_finalized.columns:\n",
    "            df_finalized[col] = df_finalized[col].astype('category')\n",
    "    \n",
    "    # Convierte las columnas de conteo a entero\n",
    "    for col in count_cols:\n",
    "         if col in df_finalized.columns:\n",
    "            df_finalized[col] = df_finalized[col].astype(int)\n",
    "\n",
    "    # Reinicia el index\n",
    "    df_finalized = df_finalized.reset_index(drop=True)\n",
    "\n",
    "    print(\"Tipos de datos finalizados.\")\n",
    "    return df_finalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5f84c",
   "metadata": {},
   "source": [
    "### 5. EJECUCIÓN DE PIPELINE COMPLETO DE LIMPIEZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9f087ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cleaning_pipeline(raw_data_path: str, expected_columns: list) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline completo de limpieza de datos sobre un archivo CSV.\n",
    "\n",
    "    Args:\n",
    "        raw_data_path (str): La ruta al archivo CSV crudo.\n",
    "        expected_columns (list): La lista de columnas esperadas en el dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Un DataFrame limpio y procesado, o None si la carga inicial falla.\n",
    "    \"\"\"\n",
    "    print(\"INICIANDO PIPELINE DE LIMPIEZA DE DATOS\")\n",
    "    \n",
    "    # --- PASO 1: CARGA ---\n",
    "    print(\"\\n--- PASO 1: CARGA DEL DATASET CRUDO ---\")\n",
    "    df = load_data(raw_data_path)\n",
    "    \n",
    "    if df is None:\n",
    "        return None # Retorna None si la carga falla\n",
    "\n",
    "    # --- PASO 2: APLICANDO TRANSFORMACIONES ---\n",
    "    # Se encadenan todas las funciones de limpieza de forma secuencial.\n",
    "    print(\"\\n--- PASO 2: APLICANDO TRANSFORMACIONES DE LIMPIEZA ---\")\n",
    "    df = drop_unnecessary_columns(df, expected_columns)\n",
    "    df = correct_initial_data_types(df)\n",
    "    df = handle_invalid_values(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = drop_duplicate_rows(df)\n",
    "    df = check_inconsistencies(df)\n",
    "    df = finalize_data_types(df) # El resultado de esta es el df final\n",
    "    \n",
    "    print(\"\\nPipeline de limpieza completado exitosamente.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0703465",
   "metadata": {},
   "source": [
    "### 6. EXPORTACIÓN DE DATAFRAME A .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "309b9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Pandas en un archivo CSV.\n",
    "\n",
    "    Verifica si el directorio de destino existe y lo crea si es necesario.\n",
    "    Guarda el archivo sin el índice de Pandas.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame que se va a guardar.\n",
    "        file_path (str): La ruta completa del archivo, incluyendo el nombre y la extensión .csv.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extrae el directorio de la ruta del archivo\n",
    "        directory = os.path.dirname(file_path)\n",
    "        \n",
    "        # Crea el directorio si no existe\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"\\nDirectorio creado: {directory}\")\n",
    "            \n",
    "        # Guarda el DataFrame en CSV sin el índice\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"\\nDataFrame guardado exitosamente en: {file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError al guardar el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b169400",
   "metadata": {},
   "source": [
    "### FUNCIÓN MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0655b69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INICIANDO PIPELINE DE LIMPIEZA DE DATOS\n",
      "\n",
      "--- PASO 1: CARGA DEL DATASET CRUDO ---\n",
      "Datos cargados exitosamente desde: ../data/raw/bike_sharing_modified.csv\n",
      "\n",
      "--- PASO 2: APLICANDO TRANSFORMACIONES DE LIMPIEZA ---\n",
      "\n",
      "Columnas a eliminar: ['instant', 'mixed_type_col']\n",
      "Columnas innecesarias eliminadas.\n",
      "\n",
      "Iniciando corrección de tipos de datos...\n",
      "Tipos de datos corregidos de forma semántica.\n",
      "\n",
      "Iniciando validación de valores...\n",
      "  -> Columna 'season': 168 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'yr': 136 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'mnth': 177 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'hr': 174 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'holiday': 96 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'weekday': 158 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'workingday': 120 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'weathersit': 180 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'hum': 167 valores inválidos convertidos a NaN.\n",
      "  -> Columna 'windspeed': 160 valores inválidos convertidos a NaN.\n",
      "Validación de valores completada.\n",
      "\n",
      "Iniciando manejo de valores nulos...\n",
      "\n",
      "  -> Paso 1: Eliminando filas con datos críticos faltantes...\n",
      "Se eliminaron 2046 filas.\n",
      "\n",
      "  -> Paso 2: Imputando valores restantes de forma contextual...\n",
      "Proceso de imputación de nulos finalizado.\n",
      "\n",
      "Verificando filas duplicadas...\n",
      "Se eliminaron 214 filas duplicadas.\n",
      "\n",
      "Verificando inconsistencias lógicas...\n",
      "\n",
      "  -> Validando consistencia de fecha (yr, mnth, season, weekday)...\n",
      "    - Corregidas 0 inconsistencias en 'yr'.\n",
      "    - Corregidas 0 inconsistencias en 'mnth'.\n",
      "    - Corregidas 0 inconsistencias en 'weekday'.\n",
      "    - Corregidas 0 inconsistencias en 'season'.\n",
      "\n",
      "  -> Validando consistencia de 'workingday'...\n",
      "    - Corregidas 0 inconsistencias en 'workingday'.\n",
      "\n",
      "  -> Validando consistencia de 'cnt' vs 'casual' + 'registered'...\n",
      "    - Se eliminaron 433 filas por inconsistencia en la suma de conteos.\n",
      "\n",
      "Verificación de inconsistencias completada.\n",
      "\n",
      "Puliendo los tipos de datos finales...\n",
      "Tipos de datos finalizados.\n",
      "\n",
      "Pipeline de limpieza completado exitosamente.\n",
      "\n",
      "==========================================================\n",
      "REPORTE FINAL DEL DATAFRAME LIMPIO\n",
      "==========================================================\n",
      "\n",
      "Información General y Tipos de Datos Finales:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15033 entries, 0 to 15032\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   dteday      15033 non-null  datetime64[ns]\n",
      " 1   season      15033 non-null  category      \n",
      " 2   yr          15033 non-null  category      \n",
      " 3   mnth        15033 non-null  category      \n",
      " 4   hr          15033 non-null  category      \n",
      " 5   holiday     15033 non-null  category      \n",
      " 6   weekday     15033 non-null  category      \n",
      " 7   workingday  15033 non-null  category      \n",
      " 8   weathersit  15033 non-null  category      \n",
      " 9   temp        15033 non-null  float64       \n",
      " 10  atemp       15033 non-null  float64       \n",
      " 11  hum         15033 non-null  float64       \n",
      " 12  windspeed   15033 non-null  float64       \n",
      " 13  casual      15033 non-null  int64         \n",
      " 14  registered  15033 non-null  int64         \n",
      " 15  cnt         15033 non-null  int64         \n",
      "dtypes: category(8), datetime64[ns](1), float64(4), int64(3)\n",
      "memory usage: 1.0 MB\n",
      "\n",
      "Resumen Estadístico Final:\n",
      "              count unique   top     freq                           mean  \\\n",
      "dteday        15033    NaN   NaN      NaN  2012-01-01 20:14:53.713829376   \n",
      "season      15033.0    4.0   3.0   3873.0                            NaN   \n",
      "yr          15033.0    2.0   1.0   7554.0                            NaN   \n",
      "mnth        15033.0   12.0   3.0   1302.0                            NaN   \n",
      "hr          15033.0   24.0  14.0    648.0                            NaN   \n",
      "holiday     15033.0    2.0   0.0  14597.0                            NaN   \n",
      "weekday     15033.0    7.0   6.0   2165.0                            NaN   \n",
      "workingday  15033.0    2.0   1.0  10275.0                            NaN   \n",
      "weathersit  15033.0    4.0   1.0  10016.0                            NaN   \n",
      "temp        15033.0    NaN   NaN      NaN                       3.421889   \n",
      "atemp       15033.0    NaN   NaN      NaN                       3.628229   \n",
      "hum         15033.0    NaN   NaN      NaN                       0.626932   \n",
      "windspeed   15033.0    NaN   NaN      NaN                       0.189321   \n",
      "casual      15033.0    NaN   NaN      NaN                      35.525378   \n",
      "registered  15033.0    NaN   NaN      NaN                     153.884521   \n",
      "cnt         15033.0    NaN   NaN      NaN                     189.409898   \n",
      "\n",
      "                            min                  25%                  50%  \\\n",
      "dteday      2011-01-01 00:00:00  2011-07-03 00:00:00  2012-01-02 00:00:00   \n",
      "season                      NaN                  NaN                  NaN   \n",
      "yr                          NaN                  NaN                  NaN   \n",
      "mnth                        NaN                  NaN                  NaN   \n",
      "hr                          NaN                  NaN                  NaN   \n",
      "holiday                     NaN                  NaN                  NaN   \n",
      "weekday                     NaN                  NaN                  NaN   \n",
      "workingday                  NaN                  NaN                  NaN   \n",
      "weathersit                  NaN                  NaN                  NaN   \n",
      "temp                       0.02                 0.34                  0.5   \n",
      "atemp                       0.0               0.3333               0.4848   \n",
      "hum                         0.0                 0.48                 0.63   \n",
      "windspeed                   0.0               0.1045               0.1642   \n",
      "casual                      0.0                  4.0                 17.0   \n",
      "registered                  0.0                 34.0                116.0   \n",
      "cnt                         1.0                 40.0                143.0   \n",
      "\n",
      "                            75%                  max         std  \n",
      "dteday      2012-07-02 00:00:00  2012-12-31 00:00:00         NaN  \n",
      "season                      NaN                  NaN         NaN  \n",
      "yr                          NaN                  NaN         NaN  \n",
      "mnth                        NaN                  NaN         NaN  \n",
      "hr                          NaN                  NaN         NaN  \n",
      "holiday                     NaN                  NaN         NaN  \n",
      "weekday                     NaN                  NaN         NaN  \n",
      "workingday                  NaN                  NaN         NaN  \n",
      "weathersit                  NaN                  NaN         NaN  \n",
      "temp                       0.66               983.62   43.370978  \n",
      "atemp                    0.6212             985.4545   45.163746  \n",
      "hum                        0.78                  1.0    0.190893  \n",
      "windspeed                0.2537               0.8507    0.121324  \n",
      "casual                     48.0                367.0   49.229377  \n",
      "registered                219.0                886.0  151.145189  \n",
      "cnt                       280.0                977.0  181.090559  \n",
      "\n",
      "Primeras 5 Filas del Dataset Limpio:\n",
      "      dteday season yr mnth   hr holiday weekday workingday weathersit  temp  \\\n",
      "0 2011-01-01      1  0    1  0.0     0.0       6          0        1.0  0.24   \n",
      "1 2011-01-01      1  0    1  1.0     0.0       6          0        1.0  0.22   \n",
      "2 2011-01-01      1  0    1  2.0     0.0       6          0        1.0  0.22   \n",
      "3 2011-01-01      1  0    1  3.0     0.0       6          0        1.0  0.24   \n",
      "4 2011-01-01      1  0    1  4.0     0.0       6          0        1.0  0.24   \n",
      "\n",
      "    atemp   hum  windspeed  casual  registered  cnt  \n",
      "0  0.2879  0.81        0.0       3          13   16  \n",
      "1  0.2727  0.80        0.0       8          32   40  \n",
      "2  0.2727  0.80        0.0       5          27   32  \n",
      "3  0.2879  0.75        0.0       3          10   13  \n",
      "4  0.2879  0.75        0.0       0           1    1  \n",
      "\n",
      "DataFrame guardado exitosamente en: ../data/interim/bike_sharing_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que configura y ejecuta el pipeline de limpieza,\n",
    "    y luego presenta un reporte del resultado.\n",
    "    \"\"\"\n",
    "    # --- 0. CONFIGURACIÓN ---\n",
    "    RAW_DATA_PATH = '../data/raw/bike_sharing_modified.csv'\n",
    "    EXPECTED_COLUMNS = [\n",
    "        'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', \n",
    "        'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', \n",
    "        'casual', 'registered', 'cnt'\n",
    "    ]\n",
    "    \n",
    "    # --- 1. EJECUCIÓN DEL PIPELINE ---\n",
    "    cleaned_df = run_cleaning_pipeline(\n",
    "        raw_data_path=RAW_DATA_PATH,\n",
    "        expected_columns=EXPECTED_COLUMNS\n",
    "    )\n",
    "    \n",
    "    # --- 2. REPORTE FINAL ---\n",
    "    if cleaned_df is not None:\n",
    "        print(\"\\n==========================================================\")\n",
    "        print(\"REPORTE FINAL DEL DATAFRAME LIMPIO\")\n",
    "        print(\"==========================================================\")\n",
    "        \n",
    "        print(\"\\nInformación General y Tipos de Datos Finales:\")\n",
    "        cleaned_df.info()\n",
    "        \n",
    "        print(\"\\nResumen Estadístico Final:\")\n",
    "        print(cleaned_df.describe(include='all').T)\n",
    "        \n",
    "        print(\"\\nPrimeras 5 Filas del Dataset Limpio:\")\n",
    "        print(cleaned_df.head())\n",
    "\n",
    "        # --- GUARDADO DEL RESULTADO ---\n",
    "        CLEANED_DATA_PATH = '../data/interim/bike_sharing_cleaned.csv'\n",
    "        \n",
    "        # Se llama a la función para guardar el archivo\n",
    "        save_dataframe_to_csv(df=cleaned_df, file_path=CLEANED_DATA_PATH)\n",
    "    else:\n",
    "        print(\"El pipeline no generó un resultado debido a un error en la carga de datos.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Punto de Entrada del Script\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
