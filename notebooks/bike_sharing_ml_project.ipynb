{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f788e2",
   "metadata": {},
   "source": [
    "### 1. IMPORTACIÓN DE LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9309b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761524a9",
   "metadata": {},
   "source": [
    "### 2. CARGA DE LOS DATOS (DE MOMENTO CONSIDERANDO UN .CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57dad4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Carga un conjunto de datos desde una ruta de archivo CSV.\n",
    "\n",
    "    Esta función encapsula la lógica de lectura de datos con Pandas,\n",
    "    incluyendo un manejo básico de errores si el archivo no se encuentra.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): La ruta al archivo .csv que se va a cargar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Un DataFrame de Pandas con los datos cargados,\n",
    "        o None si ocurre un error (ej. archivo no encontrado).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intenta leer el archivo CSV y lo carga en un DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Datos cargados exitosamente desde: {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        # Manejo de error si el archivo no existe en la ruta especificada\n",
    "        print(f\"Error: El archivo no fue encontrado en la ruta: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Manejo de otros posibles errores durante la carga\n",
    "        print(f\"Ocurrió un error inesperado al cargar el archivo: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175ed98",
   "metadata": {},
   "source": [
    "### 3. DIAGNÓSTICO INICIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18d824d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_overview(df: pd.DataFrame, df_name: str = \"DataFrame\") -> None:\n",
    "    \"\"\"\n",
    "    Imprime un resumen completo y diagnóstico de un DataFrame.\n",
    "\n",
    "    Incluye dimensiones, tipos de datos, estadísticas descriptivas,\n",
    "    conteo de duplicados y porcentaje de valores nulos.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame que se va a analizar.\n",
    "        df_name (str): Un nombre opcional para el DataFrame que se mostrará en los reportes.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===============ANÁLISIS EXPLORATORIO RÁPIDO PARA: '{df_name}'===============\")\n",
    "\n",
    "    # 1. Dimensiones del DataFrame\n",
    "    print(f\"\\n**Dimensiones:** {df.shape[0]} filas y {df.shape[1]} columnas.\")\n",
    "\n",
    "    # 2. Tipos de datos y valores no nulos\n",
    "    print(\"\\n**Tipos de Datos y Valores No Nulos:**\")\n",
    "    df.info()\n",
    "\n",
    "    # 3. Estadísticas Descriptivas para variables numéricas\n",
    "    print(\"\\n**Estadísticas Descriptivas (Numéricas):**\")\n",
    "    # Usamos .T para transponer la tabla y hacerla más legible\n",
    "    print(df.describe().T)\n",
    "\n",
    "    # 4. Conteo de filas duplicadas\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\n**Filas Duplicadas:** {duplicates} filas duplicadas encontradas.\")\n",
    "\n",
    "    # 5. Porcentaje de Valores Nulos por columna\n",
    "    null_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    null_info = null_percentage[null_percentage > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if not null_info.empty:\n",
    "        print(\"\\n**Porcentaje de Valores Nulos (>0%):**\")\n",
    "        print(null_info)\n",
    "    else:\n",
    "        print(\"\\n**No se encontraron valores nulos.**\")\n",
    "    \n",
    "    print(\"\\nFin del análisis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c8d8e",
   "metadata": {},
   "source": [
    "### 4. LIMPIEZA DE LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afb0e9",
   "metadata": {},
   "source": [
    "##### Validación del esquema. Eliminación de columnas no identificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49d0d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary_columns(df: pd.DataFrame, valid_columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Elimina las columnas de un DataFrame que no están en una lista de columnas válidas.\n",
    "\n",
    "    Esta función es útil para asegurar que el DataFrame solo contenga las columnas\n",
    "    esperadas según el esquema definido.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame a limpiar.\n",
    "        valid_columns (list): Una lista de strings con los nombres de las\n",
    "                              columnas que deben permanecer.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nuevo DataFrame que solo contiene las columnas válidas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identifica las columnas actuales del DataFrame\n",
    "    current_columns = df.columns.tolist()\n",
    "    \n",
    "    # Encuentra las columnas que están en el DataFrame pero no en la lista de válidas\n",
    "    cols_to_drop = [col for col in current_columns if col not in valid_columns]\n",
    "\n",
    "    if cols_to_drop:\n",
    "        print(f\"\\nColumnas a eliminar: {cols_to_drop}\")\n",
    "        # Elimina las columnas identificadas y devuelve una copia del df modificado\n",
    "        df_cleaned = df.drop(columns=cols_to_drop)\n",
    "        print(\"Columnas innecesarias eliminadas.\")\n",
    "    else:\n",
    "        print(\"No se encontraron columnas innecesarias. El esquema es correcto.\")\n",
    "        df_cleaned = df.copy() # Devuelve una copia para mantener la consistencia\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603e467",
   "metadata": {},
   "source": [
    "##### Conversión del tipo correcto de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f3e0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_initial_data_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Corrige los tipos de datos de columnas específicas a numérico y fecha.\n",
    "    Las categóricas de momento igual se consideran como numéricas, para facilitar el manejo de inválidos.\n",
    "\n",
    "    Usa 'errors=coerce' para convertir valores no válidos en NaN,\n",
    "    facilitando su manejo posterior.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame a procesar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nuevo DataFrame con los tipos de datos corregidos.\n",
    "    \"\"\"\n",
    "\n",
    "    df_corrected = df.copy()\n",
    "    print(\"\\nIniciando corrección de tipos de datos...\")\n",
    "\n",
    "    # Define los grupos de columnas\n",
    "    numeric_cols = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt',\n",
    "                    'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "    \n",
    "    # 1. Procesa columnas numéricas\n",
    "    for col in numeric_cols:\n",
    "        # Omite si la columna no existe (por si fue eliminada antes)\n",
    "        if col in df_corrected.columns:\n",
    "            df_corrected[col] = pd.to_numeric(df_corrected[col], errors='coerce')\n",
    "\n",
    "    # 2. Procesa columna de fecha\n",
    "    if 'dteday' in df_corrected.columns:\n",
    "        # Usamos format='mixed' para manejar explícitamente los formatos múltiples\n",
    "        df_corrected['dteday'] = pd.to_datetime(\n",
    "            df_corrected['dteday'], \n",
    "            errors='coerce', \n",
    "            format='mixed' # Esta es la solución\n",
    "        )\n",
    "        \n",
    "    print(\"Tipos de datos corregidos de forma semántica.\")\n",
    "    return df_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19660c0",
   "metadata": {},
   "source": [
    "##### Manejo de valores no válidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b39495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_invalid_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Valida los datos contra un conjunto de reglas y convierte los inválidos a NaN.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame con tipos de datos ya corregidos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nuevo DataFrame con los valores inválidos convertidos a NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    df_validated = df.copy()\n",
    "    print(\"\\nIniciando validación de valores...\")\n",
    "\n",
    "    # Diccionario de reglas de validación\n",
    "    # Para categóricas: lista de valores permitidos\n",
    "    # Para numéricas: tupla con (valor_mínimo, valor_máximo)\n",
    "    validation_rules = {\n",
    "        'dteday': (pd.to_datetime('2011-01-01'), pd.to_datetime('2012-12-31')),\n",
    "        'season': [1, 2, 3, 4],\n",
    "        'yr': [0, 1],\n",
    "        'mnth': list(range(1, 13)),\n",
    "        'hr': list(range(0, 24)),\n",
    "        'holiday': [0, 1],\n",
    "        'weekday': list(range(0, 7)),\n",
    "        'workingday': [0, 1],\n",
    "        'weathersit': [1, 2, 3, 4],\n",
    "        'hum': (0.0, 1.0),\n",
    "        'windspeed': (0.0, 1.0),\n",
    "        'cnt': (0, float('inf')) # El conteo no puede ser negativo\n",
    "    }\n",
    "\n",
    "    for column, rule in validation_rules.items():\n",
    "        if column in df_validated.columns:\n",
    "            # Conteo inicial de nulos para reporte\n",
    "            initial_nulls = df_validated[column].isnull().sum()\n",
    "\n",
    "            # Validación para variables categóricas (regla es una lista)\n",
    "            if isinstance(rule, list):\n",
    "                invalid_mask = ~df_validated[column].isin(rule)\n",
    "            # Validación para variables numéricas (regla es una tupla)\n",
    "            elif isinstance(rule, tuple):\n",
    "                min_val, max_val = rule\n",
    "                invalid_mask = (df_validated[column] < min_val) | (df_validated[column] > max_val)\n",
    "            \n",
    "            # Reemplaza los valores que no cumplen la regla con NaN\n",
    "            df_validated.loc[invalid_mask, column] = np.nan\n",
    "            \n",
    "            # Reporta cuántos valores inválidos se encontraron y corrigieron\n",
    "            final_nulls = df_validated[column].isnull().sum()\n",
    "            newly_invalid = final_nulls - initial_nulls\n",
    "            if newly_invalid > 0:\n",
    "                print(f\"  -> Columna '{column}': {newly_invalid} valores inválidos convertidos a NaN.\")\n",
    "\n",
    "    print(\"Validación de valores completada.\")\n",
    "    return df_validated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842592d",
   "metadata": {},
   "source": [
    "##### Imputación de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4cfde046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_season(date_obj: pd.Timestamp) -> int:\n",
    "    \"\"\"\n",
    "    Convierte una fecha completa a la estación correspondiente de forma precisa,\n",
    "    considerando los días de corte (solsticios y equinoccios).\n",
    "    Dataset: 1:invierno, 2:primavera, 3:verano, 4:otoño.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_obj):\n",
    "        return np.nan\n",
    "        \n",
    "    month = date_obj.month\n",
    "    day = date_obj.day\n",
    "\n",
    "    # Invierno: Desde 21 de Dic hasta 20 de Mar\n",
    "    if (month == 12 and day >= 21) or (month in [1, 2]) or (month == 3 and day < 21):\n",
    "        return 1\n",
    "    # Primavera: Desde 21 de Mar hasta 20 de Jun\n",
    "    elif (month == 3 and day >= 21) or (month in [4, 5]) or (month == 6 and day < 21):\n",
    "        return 2\n",
    "    # Verano: Desde 21 de Jun hasta 22 de Sep\n",
    "    elif (month == 6 and day >= 21) or (month in [7, 8]) or (month == 9 and day < 23):\n",
    "        return 3\n",
    "    # Otoño: Desde 23 de Sep hasta 20 de Dic\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9a3b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gestiona valores nulos: primero elimina filas irrecuperables y luego\n",
    "    imputa los nulos restantes de forma contextual.\n",
    "    \"\"\"\n",
    "\n",
    "    df_processed = df.copy()\n",
    "    print(\"\\nIniciando manejo de valores nulos...\")\n",
    "    \n",
    "    initial_rows = len(df_processed)\n",
    "    \n",
    "    # --- ESTRATEGIA 1: ELIMINACIÓN DE FILAS CRÍTICAS ---\n",
    "    print(\"\\n  -> Paso 1: Eliminando filas con datos críticos faltantes...\")\n",
    "    critical_cols = ['dteday', 'hr', 'holiday', 'workingday', 'casual', 'registered', 'cnt']\n",
    "    df_processed.dropna(subset=critical_cols, inplace=True)\n",
    "    rows_deleted = initial_rows - len(df_processed)\n",
    "    if rows_deleted > 0:\n",
    "        print(f\"Se eliminaron {rows_deleted} filas.\")\n",
    "\n",
    "    # --- ESTRATEGIA 2: IMPUTACIÓN CONTEXTUAL ---\n",
    "    print(\"\\n  -> Paso 2: Imputando valores restantes de forma contextual...\")\n",
    "    \n",
    "    # Derivación por fecha (solo para celdas vacías)\n",
    "    mask_yr = df_processed['yr'].isna()\n",
    "    df_processed.loc[mask_yr, 'yr'] = df_processed.loc[mask_yr, 'dteday'].dt.year - 2011\n",
    "    \n",
    "    mask_mnth = df_processed['mnth'].isna()\n",
    "    df_processed.loc[mask_mnth, 'mnth'] = df_processed.loc[mask_mnth, 'dteday'].dt.month\n",
    "    \n",
    "    mask_weekday = df_processed['weekday'].isna()\n",
    "    df_processed.loc[mask_weekday, 'weekday'] = (df_processed.loc[mask_weekday, 'dteday'].dt.weekday + 1) % 7\n",
    "    \n",
    "    mask_season = df_processed['season'].isna()\n",
    "    df_processed.loc[mask_season, 'season'] = df_processed.loc[mask_season, 'dteday'].apply(date_to_season)\n",
    "\n",
    "    # Imputación estadística para el clima (sin cambios)\n",
    "    weather_cols = ['weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "    for column in weather_cols:\n",
    "        if column in df_processed.columns and df_processed[column].isnull().any():\n",
    "            median_val = df_processed[column].median()\n",
    "            df_processed[column] = df_processed[column].fillna(median_val)\n",
    "\n",
    "    print(\"Proceso de imputación de nulos finalizado.\")\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f513c1",
   "metadata": {},
   "source": [
    "##### Revisión de registros con inconsistencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d81ffc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inconsistencies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Verifica y corrige inconsistencias lógicas en el DataFrame.\n",
    "\n",
    "    - Regla 1: Valida 'season', 'yr', 'mnth', 'weekday' contra 'dteday'.\n",
    "    - Regla 2: Valida 'workingday' contra 'weekday' y 'holiday'.\n",
    "    - Regla 3: Valida 'cnt' = 'casual' + 'registered' y elimina filas inconsistentes.\n",
    "    \"\"\"\n",
    "\n",
    "    df_consistent = df.copy()\n",
    "    print(\"\\nVerificando inconsistencias lógicas...\")\n",
    "\n",
    "    # --- Regla 1: Consistencia de variables de tiempo vs. 'dteday' ---\n",
    "    print(\"\\n  -> Validando consistencia de fecha (yr, mnth, season, weekday)...\")\n",
    "    \n",
    "    # Se calculan los valores correctos a partir de la fecha\n",
    "    correct_yr = df_consistent['dteday'].dt.year - 2011\n",
    "    correct_mnth = df_consistent['dteday'].dt.month\n",
    "    correct_weekday = (df_consistent['dteday'].dt.weekday + 1) % 7\n",
    "    correct_season = df_consistent['dteday'].apply(date_to_season)\n",
    "\n",
    "    # Se comparan y corrigen\n",
    "    yr_inconsistencies = (df_consistent['yr'] != correct_yr).sum()\n",
    "    mnth_inconsistencies = (df_consistent['mnth'] != correct_mnth).sum()\n",
    "    weekday_inconsistencies = (df_consistent['weekday'] != correct_weekday).sum()\n",
    "    season_inconsistencies = (df_consistent['season'] != correct_season).sum()\n",
    "    \n",
    "    df_consistent['yr'] = correct_yr\n",
    "    df_consistent['mnth'] = correct_mnth\n",
    "    df_consistent['weekday'] = correct_weekday\n",
    "    df_consistent['season'] = correct_season\n",
    "    \n",
    "    print(f\"    - Corregidas {yr_inconsistencies} inconsistencias en 'yr'.\")\n",
    "    print(f\"    - Corregidas {mnth_inconsistencies} inconsistencias en 'mnth'.\")\n",
    "    print(f\"    - Corregidas {weekday_inconsistencies} inconsistencias en 'weekday'.\")\n",
    "    print(f\"    - Corregidas {season_inconsistencies} inconsistencias en 'season'.\")\n",
    "    \n",
    "    # --- Regla 2: Consistencia de 'workingday' ---\n",
    "    print(\"\\n  -> Validando consistencia de 'workingday'...\")\n",
    "    \n",
    "    # Se calcula el valor correcto: no es fin de semana (0 o 6) Y no es festivo (0)\n",
    "    correct_workingday = ((df_consistent['weekday'].isin([0, 6])) | (df_consistent['holiday'] == 1)).apply(lambda x: 0 if x else 1)\n",
    "    \n",
    "    workingday_inconsistencies = (df_consistent['workingday'] != correct_workingday).sum()\n",
    "    df_consistent['workingday'] = correct_workingday\n",
    "    print(f\"    - Corregidas {workingday_inconsistencies} inconsistencias en 'workingday'.\")\n",
    "\n",
    "    # --- Regla 3: Consistencia de los conteos ('cnt') ---\n",
    "    print(\"\\n  -> Validando consistencia de 'cnt' vs 'casual' + 'registered'...\")\n",
    "    \n",
    "    # Se identifican las filas donde la suma no cuadra\n",
    "    inconsistent_sum_mask = df_consistent['cnt'] != (df_consistent['casual'] + df_consistent['registered'])\n",
    "    \n",
    "    rows_to_drop = inconsistent_sum_mask.sum()\n",
    "    \n",
    "    if rows_to_drop > 0:\n",
    "        df_consistent = df_consistent[~inconsistent_sum_mask]\n",
    "        print(f\"    - Se eliminaron {rows_to_drop} filas por inconsistencia en la suma de conteos.\")\n",
    "    else:\n",
    "        print(\"    - No se encontraron inconsistencias en la suma de conteos.\")\n",
    "\n",
    "    print(\"\\nVerificación de inconsistencias completada.\")\n",
    "    return df_consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609554d1",
   "metadata": {},
   "source": [
    "##### Eliminación de registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87e518b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encuentra y elimina filas completamente duplicadas en el DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\nVerificando filas duplicadas...\")\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # Se eliminan las filas duplicadas\n",
    "    df_unique = df.drop_duplicates()\n",
    "    \n",
    "    final_rows = len(df_unique)\n",
    "    rows_dropped = initial_rows - final_rows\n",
    "    \n",
    "    if rows_dropped > 0:\n",
    "        print(f\"Se eliminaron {rows_dropped} filas duplicadas.\")\n",
    "    else:\n",
    "        print(\"No se encontraron filas duplicadas.\")\n",
    "        \n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d99616",
   "metadata": {},
   "source": [
    "##### Conversión final de columnas categóricas y numéricas enteras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87d2a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_data_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Paso Final: Convierte las columnas a sus tipos semánticos finales (category, int).\n",
    "    Se ejecuta después de que toda la limpieza e imputación han sido completadas.\n",
    "    \"\"\"\n",
    "    df_finalized = df.copy()\n",
    "    print(\"\\nPuliendo los tipos de datos finales...\")\n",
    "\n",
    "    # Define qué columnas deben ser categóricas y cuáles de conteo (enteros)\n",
    "    categorical_cols = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "    count_cols = ['casual', 'registered', 'cnt']\n",
    "\n",
    "    # Convierte las columnas categóricas\n",
    "    for col in categorical_cols:\n",
    "        if col in df_finalized.columns:\n",
    "            df_finalized[col] = df_finalized[col].astype('category')\n",
    "    \n",
    "    # Convierte las columnas de conteo a entero\n",
    "    for col in count_cols:\n",
    "         if col in df_finalized.columns:\n",
    "            df_finalized[col] = df_finalized[col].astype(int)\n",
    "\n",
    "    # Reinicia el index\n",
    "    df_finalized = df_finalized.reset_index(drop=True)\n",
    "\n",
    "    print(\"Tipos de datos finalizados.\")\n",
    "    return df_finalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
